setup:
  nb_timesteps: &nb_timesteps 100000
  checkpoint_freq: 10
  checkpoint_at_end: True
  keep_checkpoints_num: 5
  checkpoint_score_attr: evaluation/episode_reward_mean
  verbose: 1
  storage_path: /Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/ # /home3/s3374610/mahrl_grid2op/runs/ # 
  experiment_name: replicate_single_agent
  save_artifact: True
  num_samples: 1

debugging:
  seed: &seed 21

training:
  # tunable parameters
  gamma: 0.99
  lr: 0.0001
  kl_coeff: 0.2
  entropy_coeff: 0
  clip_param: 0.3
  num_sgd_iter: 5
  sgd_minibatch_size: 256
  train_batch_size: 1024
  model:
    fcnet_hiddens: 
    - 256
    - 256
    - 256
    fcnet_activation: relu
  _enable_learner_api: False
  batch_mode: complete_episodes

rl_module:
  _enable_rl_module_api: False

# explore:
#   exploration_config:
#     type: StochasticSampling

callbacks:
  callbacks: !SingleAgentCallback

framework:
  framework: torch

environment:
  env_config: &env_config_train
    env_name: rte_case14_realistic_train
    # num_agents: 3
    action_space: medha_DN_onechange
    lib_dir: /Users/barberademol/Documents/GitHub/mahrl_grid2op/ #  /home3/s3374610/mahrl_grid2op/ 
    max_tsteps: *nb_timesteps
    grid2op_kwargs:
      # general kwargs
      reward_class: !ScaledL2RPNReward
    seed: *seed
    rho_threshold: 0.95

env_config_val: &env_config_val
  <<: *env_config_train
  env_name: rte_case14_realistic_val

evaluation:
  evaluation_interval: 3
  evaluation_duration: 200 # as many as there are validation episodes
  evaluation_duration_unit: episodes
  evaluation_config: 
    env_config: *env_config_val # use the validation environment
    explore: True

# resources:
#   num_gpus: 0
#   num_learner_workers: 4

# rollouts:
#   num_rollout_workers: 4