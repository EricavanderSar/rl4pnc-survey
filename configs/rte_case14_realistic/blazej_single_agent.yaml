setup:
  nb_timesteps: &nb_timesteps 100000
  checkpoint_freq: 2
  verbose: 1
  storage_path: /Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/ # /home3/s3374610/mahrl_grid2op/runs/ # 
  experiment_name: rte_case14_realistic/single_agent/replicate_blazej
  save_artifact: True
  num_samples: 2

debugging:
  seed: &seed 20

training:
  # tunable parameters
  gamma: 0.99 # !grid_search [0.95, 0.99]
  lr: 0.0001 # !grid_search [0.00005, 0.0005, 0.005]
  # vf_loss_coeff: 0.5
  kl_coeff: 0.2
  entropy_coeff: 0
  clip_param: 0.3
  # lambda_: 0.95
  num_sgd_iter: 15
  sgd_minibatch_size: 256 # !grid_search [32, 64, 128]
  train_batch_size: 1024
  model:
    fcnet_hiddens: 
    - 256
    - 256
    - 256
    fcnet_activation: relu
  _enable_learner_api: False
  batch_mode: complete_episodes
  # lr: 0.0001 #!choice [0.001, 0.0001] #tune.grid_search([1e-3 1e-41e-5])
  # kl_coeff: 0.2 # !choice [0.15, 0.3, 0.2, 0.25]
  # lambda:  0.95 # !quniform [0.94, 0.96, 0.01] 
  # vf_loss_coeff: 0.9 #!choice [0.7, 0.9] # 0.9 #!quniform [0.75,1,0.05]
  # vf_clip_param: 900 #!choice [900, 1200, 1500] # 1500 #!choice [100, 500, 1500, 2000]
  # rollout_fragment_length: 128 #!choice [64,128,200] # 16
  # sgd_minibatch_size: 256 #!choice [256,512] # 64
  # train_batch_size: 2048 #!grid_search [1024, 4096] #!choice [1024, 2048] #2048
  # num_sgd_iter: 15 #!choice [10,15] #!choice [2,5,10,20,40]
  # entropy_coeff: 0.01 #!choice [0.01, 0.03, 0.05] #!choice [0.01, 0.05, 0.1, 0.15]

rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: StochasticSampling

callbacks:
  callbacks: !SingleAgentCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]
  count_steps_by: environment_steps

environment:
  env_config: &env_config_train
    env_name: rte_case14_realistic
    num_agents: 3
    action_space: medha
    lib_dir: /Users/barberademol/Documents/GitHub/mahrl_grid2op/ #  /home3/s3374610/mahrl_grid2op/ 
    max_tsteps: *nb_timesteps
    grid2op_kwargs:
      # general kwargs
      reward_class: !ScaledL2RPNReward
    seed: *seed
    rho_threshold: 0.95

env_config_val: &env_config_val
  <<: *env_config_train
  env_name: rte_case14_realistic_val

evaluation:
  evaluation_interval: 1
  evaluation_duration: 100 # as many as there are validation episodes
  evaluation_duration_unit: episodes
  evaluation_config: 
    env_config: *env_config_val # use the validation environment

resources:
  num_gpus: 0
  num_learner_workers: 4

rollouts:
  num_rollout_workers: 4