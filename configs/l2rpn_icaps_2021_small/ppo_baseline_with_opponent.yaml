setup:
  nb_timesteps: &nb_timesteps 1000000
  checkpoint_freq: 10000
  verbose: 1
  storage_path: ./runs

debugging:
  seed: &seed 14

training:
  # tunable parameters
  gamma: 0.95 # !grid_search [0.95, 0.99]
  lr: 0.003 # !grid_search [0.0005, 0.005, 0.05]
  vf_loss_coeff: 0.5
  entropy_coeff: 0.01
  clip_param: 0.2
  lambda_: 0.95
  sgd_minibatch_size: 32 # !grid_search [32, 64, 128]
  train_batch_size: 128
  model:
    fcnet_hiddens: 
    - 256
    - 256
  _enable_learner_api: False
  
rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: EpsilonGreedy

callbacks:
  callbacks: !CustomMetricsCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]

environment:
  env_config:
    env_name: l2rpn_icaps_2021_large
    num_agents: 3
    action_space: asymmetry
    lib_dir: /Users/barberademol/Documents/GitHub/rl4pnc/ # /home3/s3374610/rl4pnc/
    max_tsteps: *nb_timesteps
    grid2op_kwargs:
      # general kwargs
      reward_class: !LossReward
      # kwargs specific to opponents
      opponent_attack_cooldown: 12 # 1 hour
      opponent_attack_duration: 96 # 8 hours in a day
      opponent_budget_per_ts: 0.17 # (opponent_attack_duration / opponent_attack_cooldown) + epsilon
      opponent_init_budget: 144
      opponent_action_class: !PowerlineSetAction 
      opponent_class: !RandomLineOpponent
      opponent_budget_class: !BaseActionBudget
      opponent_space_type: !ReconnectingOpponentSpace
      kwargs_opponent:
        lines_attacked: 
          - "62_58_180"
          - "62_63_160"
          - "48_50_136"
          - "48_53_141"
          - "41_48_131"
          - "39_41_121"
          - "43_44_125"
          - "44_45_126"
          - "34_35_110"
          - "54_58_154"
    seed: *seed
    rho_threshold: 0.95