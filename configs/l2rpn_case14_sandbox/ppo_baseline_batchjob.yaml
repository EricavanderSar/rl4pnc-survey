setup:
  max_ep_len: 8064
  duration: 0 # HH:MM or minutes of training before stopping, overwrites nb_timesteps limit if value > 0
  nb_timesteps: &nb_timesteps 500_000 # 500.000 with opponent
  checkpoint_freq: 10
  verbose: 0
  my_log_level: 1
  storage_path: ./runs
  folder_name: "Case14_SurveyPaperRainbow_Opp" # "Case14_Opponent"
  experiment_name: "Case14_SurveyPaperRainbow_Opp"
  optimize: False
  num_samples: 32
  score_metric: evaluation/custom_metrics/grid2op_end_mean
#  grace_period: 10_000 # First 10_000 steps of training before stopping - CURRENTLY NOT USED!
#  result_dir: "/home/evandersar/ray_results/Case14_FiFo"
#  points_to_evaluate:
#    gamma: 0.985
#    lr: 0.001
#    num_sgd_iter: 4
#    sgd_minibatch_size: 64
#    train_batch_size: 256
#    model/fcnet_hiddens: [128,128,128]
#    model/fcnet_activation: relu
#    post_fcnet_hiddens: []


debugging:
  seed: &seed 14

training:
  checkpoint_trainable_policies_only: True
  gamma: 0.99 #!quniform [0.975, 0.995, 0.001] #
  lr: 0.0001 #!qloguniform [0.0001, 0.005, 0.0001] #
#  use_kl_loss: False # setting this similar to orig EVDS implementation
  kl_coeff: 0.2
  entropy_coeff: 0.01 # 0.0 # with opponent: 0.01 #!choice [0.1, 0.0] #
  clip_param:  0.3 # !choice [0.2, 0.3] # epsilon
  lambda: 0.95
  vf_loss_coeff: 0.5
  vf_clip_param: 100
  num_sgd_iter: 15 # 5 # with opponent: 15 #!quniform [4, 16, 4] #
  sgd_minibatch_size: 256 #128 #!choice [64, 128, 256] #
  train_batch_size: 1024 #256 # 128 #!choice [256, 512, 1024] #
  batch_mode: complete_episodes
  model:
#    custom_model : linfcn
    fcnet_hiddens: [256,256,256] # !choice [(128,128,128), (256,256,256), (256, 128, 256)]  # [128,128,128] #
    fcnet_activation: relu #!choice [relu, tanh] #
#    post_fcnet_hiddens: !choice [(),(128), (256)] #[256] #
  _enable_learner_api: False

rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: StochasticSampling

callbacks:
  callbacks: !CustomMetricsCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]
  count_steps_by: agent_steps

environment:
  env_config: &env_config_train
    env_type: old_env #new_env #
    env_name: l2rpn_case14_sandbox_train
    action_space: d3qn2022 # options: assym, medha, tennet, d3qn2022, masked
    mask: 3 # only used if action space is "masked" (as in SMAAC from Yoon et al. 2021)
    lib_dir: !workdir
    grid2op_kwargs:
      # general kwargs
      reward_class: !AlphaZeroRW #!ScaledL2RPNReward #L2RPNReward #!LossRwRescaled2 #!LossRwNew #!AlphaZeroRW #!LinesCapacityReward #!LossReward #!RewardRho #!ConstantReward
    seed: *seed
    rho_threshold: 0.99
    prio: False
    use_ffw: False # feed forward chronic when using prio
    g2op_input: ["p_i", "p_l", "r", "o", "t" ] #['v_l', 'a', 'r', 't'] # #"all" #all options: ["t" (topo), "l" (line status), "p_i"(active power input), "p_l" (act power lines), "q_i", "q_l", "v_i", "v_l", "theta_i", "theta_l", "a", "r", "o", "m" ]
    custom_input: [""] #options: [ "d" (danger), "t" (time of day), "y" (day of year)]
    danger: 0.9 # used when danger is in custom_input
    n_history: 1 # If n_history > 1, the observation will be a list of previous observations
    reset_topo: 0.8 # No reset topo applied when = 0
    line_reco: True # Line reconnection when possible
    line_disc: True # Line disconnection when overloaded for >1 consecutive time steps
    penalty_game_over: -300
    reward_finish: 500
    curriculum_training: False
    curriculum_thresholds: [20_000,46_667] # used if curriculum_training is True
    # Params below only used for new_env
    adj_matrix: False #use adjacency matrix instead of topology vector (is adj matrix useful for lin NN??)
    normalize: maxmin # options: maxmin, zscore, none

evaluation:
  evaluation_interval: 10
  evaluation_duration: 100 # as many as there are validation episodes -> len(os.listdir("/Users/ericavandersar/data_grid2op/rte_case14_realistic_val/chronics"))
  evaluation_duration_unit: episodes
  always_attach_evaluation_results: True
  evaluation_sample_timeout_s: 720
#  evaluation_parallel_to_training: True #Currently not working
  evaluation_num_workers: 15
  evaluation_config:
    explore: True,
    env_config:
      <<: *env_config_train
      env_name: l2rpn_case14_sandbox_val # use the validation environment
      shuffle_scenarios: False
      prio: False # should always be False
      curriculum_training: False

reporting:
  keep_per_episode_custom_metrics: True

scaling_config:
  num_workers: 4

resources:
#  num_gpus: 1
  num_learner_workers: 4

rollouts:
  num_rollout_workers: 16
