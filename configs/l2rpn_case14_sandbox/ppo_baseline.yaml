setup:
  max_ep_len: 8064
  nb_timesteps: &nb_timesteps 500
  checkpoint_freq: 2
  verbose: 0
  my_log_level: 1
  storage_path: ./runs
  folder_name: "TestSandbox14"
  experiment_name: "TestSb14"
  optimize: False
  num_samples: 3
  score_metric: evaluation/custom_metrics/grid2op_end_mean
  grace_period: 500
#  points_to_evaluate:
#    num_sgd_iter: 16
#    sgd_minibatch_size: 64
#    train_batch_size: 512
#  result_dir: /Users/ericavandersar/ray_results/Test14 #"" #

debugging:
  seed: &seed 9

training:
  checkpoint_trainable_policies_only: True
  # tunable parameters
  gamma: 0.99 #!quniform [0.95, 0.99, 0.01] #
  lr: 0.0001 # !qloguniform [0.0005, 0.005, 0.0001] #
  vf_loss_coeff: 0.5
  entropy_coeff: 0.05
  clip_param: 0.3 # epsilon
  lambda: 0.95
  vf_clip_param: 100
  use_kl_loss: False # setting this similar to orig EVDS implementation
  num_sgd_iter: 4 #spec.config.train_batch_size / spec.config.sgd_minibatch_size !quniform [4, 16, 4] #
  sgd_minibatch_size: 32 #!choice [32, 64, 128] #
  train_batch_size: 128 #80 #!choice [128, 256, 512] #
  batch_mode: complete_episodes
  model:
#    custom_model : linfcn
    fcnet_hiddens: [256,256,256] # !choice [[256,256,256], [128,128,128]] #
    fcnet_activation: relu #!choice [relu, tanh] #
    post_fcnet_hiddens: [256,128]
  _enable_learner_api: False
  
rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: StochasticSampling

callbacks:
  callbacks: !CustomMetricsCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]
  count_steps_by: agent_steps

environment:
  env_config: &env_config_train
    env_type: old_env #new_env #
#    shuffle_scenarios: True # This is not used in my version of custom_env (since I use g2op direct instead of gymenv)
    env_name: l2rpn_case14_sandbox_train
    action_space: assym
    mask: 5
    lib_dir: /Users/ericavandersar/Documents/Python_Projects/Research/rl4pnc/
    grid2op_kwargs:
      # general kwargs
      reward_class: !AlphaZeroRW
    seed: *seed
    rho_threshold: 0.90
    n_history: 6
    input: "all" #["t", "p_i", "p_l", "q_i", "q_l", "v_i", "v_l", "theta_i", "theta_l", "a", "r", "o", "d" ] # all options: ["t", "p_i", "p_l", "q_i", "q_l", "v_i", "v_l", "theta_i", "theta_l", "a", "r", "o", "d" ]
    danger: 0.9
    adj_matrix: False #use adjacency matrix instead of topology vector (is adj matrix useful for lin NN??)
    prio: False
    use_ffw: True
    normalize: maxmin
    reset_topo: 0.5

evaluation:
  evaluation_interval: 1
  evaluation_duration: 100 # as many as there are validation episodes -> len(os.listdir("/Users/ericavandersar/data_grid2op/rte_case14_realistic_val/chronics"))
  evaluation_duration_unit: episodes
  always_attach_evaluation_results: True
#  evaluation_parallel_to_training: True #Currently not working
  evaluation_num_workers: 1
  evaluation_config:
    explore: False,
    env_config:
      <<: *env_config_train
      env_name: l2rpn_case14_sandbox_val # use the validation environment
      shuffle_scenarios: False
      prio: False # should always be False

reporting:
  keep_per_episode_custom_metrics: True

resources:
  num_learner_workers: 1

rollouts:
  num_rollout_workers: 1
