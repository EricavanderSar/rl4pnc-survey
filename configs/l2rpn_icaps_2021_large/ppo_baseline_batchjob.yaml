setup:
  max_ep_len: 8062
  nb_timesteps: &nb_timesteps 200_000
  checkpoint_freq: 10
  verbose: 0
  my_log_level: 1
  storage_path: ./runs #/Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/ #/home3/s3374610/mahrl_grid2op/runs/
  folder_name: "TuneMaxRho_Sub36_2021"
  experiment_name: "TuneMaxRho_Sub36_2021"
  optimize: True
  num_samples: 25
  score_metric: evaluation/custom_metrics/grid2op_end_mean
  grace_period: 10_000 # First 10_000 steps of training before stopping
#  result_dir: "/home/evandersar/ray_results/Case14_FiFo"
#  points_to_evaluate:
#    gamma: 0.985
#    lr: 0.001
#    num_sgd_iter: 4
#    sgd_minibatch_size: 64
#    train_batch_size: 256
#    model/fcnet_hiddens: [128,128,128]
#    model/fcnet_activation: relu
#    post_fcnet_hiddens: []


debugging:
  seed: &seed 14

training:
  checkpoint_trainable_policies_only: True
  gamma: !quniform [0.97, 0.999, 0.001] #0.975 #
  lr: !qloguniform [0.00005, 0.005, 0.00001] #0.0001 #
#  use_kl_loss: False # setting this similar to orig EVDS implementation
  kl_coeff: 0.2
  entropy_coeff: 0.0
  clip_param: 0.3 #!choice [0.2, 0.3] # epsilon #
  lambda: 0.95
  vf_loss_coeff: 0.5
  vf_clip_param: 100
  num_sgd_iter: 4 #!quniform [4, 16, 4] #
  sgd_minibatch_size: 128 #!choice [64, 128, 256] #
  train_batch_size: 256 # 128 #!choice [256, 512, 1024] #
  batch_mode: complete_episodes
  model:
#    custom_model : linfcn
    fcnet_hiddens:  !choice [[256, 256], [128,128,128], [256,256,256], [512,512,512]]  # [128,128,128] #[256,256,256] #
    fcnet_activation: relu #!choice [relu, tanh] #
    post_fcnet_hiddens: !choice [[],[128], [256], [256,256], [512], [1024]] #[256] #
  _enable_learner_api: False

rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: StochasticSampling

callbacks:
  callbacks: !CustomMetricsCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]
  count_steps_by: agent_steps

environment:
  env_config: &env_config_train
    env_type: new_env #old_env #
    shuffle_scenarios: True
    env_name: l2rpn_icaps_2021_large_train
    action_space: medha_dn_maxrho1.0-100-200_gr
    mask: 3
    lib_dir: !workdir
    grid2op_kwargs:
      # general kwargs
      reward_class: !LossReward #!ScaledL2RPNReward #
    seed: *seed
    rho_threshold: 0.95
    prio: True
    input: [ "p_i", "p_l", "r", "o", "d", "m"]
    # Params below only used for new_env
    n_history: 3
    danger: 0.9
    adj_matrix: False #use adjacency matrix instead of topology vector (is adj matrix useful for lin NN??)
    normalize: maxmin

evaluation:
  evaluation_interval: 10
#  evaluation_duration: 100 # less then there are validation episodes -> len(os.listdir(f"~/data_grid2op/{env_name}/chronics"))
  evaluation_duration_unit: episodes
  always_attach_evaluation_results: True
  evaluation_sample_timeout_s: 720
#  evaluation_parallel_to_training: True #Currently not working
  evaluation_num_workers: 7
  evaluation_config:
    explore: True,
    env_config:
      <<: *env_config_train
      env_name: l2rpn_icaps_2021_large_val # use the validation environment
      shuffle_scenarios: False
      prio: False # should always be False

reporting:
  keep_per_episode_custom_metrics: True

scaling_config:
  num_workers: 4

resources:
  num_gpus: 1
  num_learner_workers: 4

rollouts:
  num_rollout_workers: 10