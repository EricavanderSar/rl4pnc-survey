setup:
  nb_timesteps: &nb_timesteps 50000
  checkpoint_freq: 0
  checkpoint_at_end: False
  # keep_checkpoints_num: 5
  checkpoint_score_attr: evaluation/episode_reward_mean
  verbose: 1
  storage_path: /Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/ #/home3/s3374610/mahrl_grid2op/runs/ 
  experiment_name: rte_case5_example/opponent_RLv_sample
  save_artifact: True
  num_samples: 2

debugging:
  seed: &seed !randint [0, 100000]

training:
  # tunable parameters
  lr: 0.0005
  num_sgd_iter: 5
  sgd_minibatch_size: 128
  train_batch_size: 512
  # lr: !grid_search [5e-3, 5e-4, 5e-5]
  # num_sgd_iter: !grid_search [5, 15]
  # sgd_minibatch_size: !grid_search [32, 64, 128] #32 # !grid_search [32, 64, 128]
  # train_batch_size: !grid_search [128, 512, 1024]
  model:
    fcnet_hiddens: 
    - 256
    - 256
  _enable_learner_api: False
  batch_mode: complete_episodes

rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: StochasticSampling

callbacks:
  callbacks: !CustomMetricsCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]
  count_steps_by: agent_steps

environment:
  env_config: &env_config_train
    env_name: rte_case5_example_per_day_train
    action_space: tennet
    lib_dir: /Users/barberademol/Documents/GitHub/mahrl_grid2op/ # /home3/s3374610/mahrl_grid2op/ 
    max_tsteps: *nb_timesteps
    grid2op_kwargs:
      # general kwargs
      reward_class: !ScaledL2RPNReward
      # kwargs specific to opponents
      opponent_attack_cooldown: 144 # max 2 attacks per day
      opponent_attack_duration: 48 # 4 hours in a day
      opponent_budget_per_ts: 0.333343333333 # taken from blazej, (opponent_attack_duration / opponent_attack_cooldown) + 1e-5
      opponent_init_budget: 144
      opponent_action_class: !PowerlineSetAction 
      opponent_class: !RandomLineOpponent
      opponent_budget_class: !BaseActionBudget
      opponent_space_type: !ReconnectingOpponentSpace #reconnecting_opponent
      kwargs_opponent:
        lines_attacked:
          - '0_3_2'
          - '0_4_3'
    rho_threshold: 0.95
    shuffle_scenarios: True
    stage: train
    prio: False

env_config_val: &env_config_val
  <<: *env_config_train
  env_name: rte_case5_example_val
  shuffle_scenarios: False
  stage: val

evaluation:
  evaluation_interval: 5
  evaluation_duration: 200 # as many as there are validation episodes, times 10 for variance
  evaluation_duration_unit: episodes
  evaluation_config: 
    env_config: *env_config_val # use the validation environment
    explore: True
  num_evaluation_workers: 1

resources:
  num_gpus: 0
  num_learner_workers: 1

rollouts:
  num_rollout_workers: 1