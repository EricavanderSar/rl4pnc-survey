# training
nb_timesteps: &nb_timesteps 1000000
checkpoint_freq: 10000
verbose: 1
gamma: 0.95 # !grid_search [0.95, 0.99]
lr: 0.003 # !grid_search [0.0005, 0.005, 0.05]
vf_loss_coeff: 0.5
entropy_coeff: 0.01
clip_param: 0.2
lambda_: 0.95
sgd_minibatch_size: 32 # !grid_search [32, 64, 128]
train_batch_size: 128
model:
  fcnet_hiddens: 
  - 256
  - 256
_enable_learner_api: False
seed: &seed 16
  
# environment
env_config:
  env_name: rte_case5_example
  num_agents: 3
  action_space: asymmetry
  lib_dir: /Users/barberademol/Documents/GitHub/mahrl_grid2op/ # /home3/s3374610/mahrl_grid2op/ 
  max_tsteps: *nb_timesteps
  grid2op_kwargs:
    # general kwargs
    test: True
    reward_class: !LossReward
    # kwargs specific to opponents
    opponent_attack_cooldown: 144 # max 2 attacks per day
    opponent_attack_duration: 48 # 4 hours in a day
    opponent_budget_per_ts: 0.333343333333 # taken from blazej, (opponent_attack_duration / opponent_attack_cooldown) + 1e-5
    opponent_init_budget: 144
    opponent_action_class: !PowerlineSetAction 
    opponent_class: !RandomLineOpponent
    opponent_budget_class: !BaseActionBudget
    opponent_space_type: !ReconnectingOpponentSpace #reconnecting_opponent
    kwargs_opponent:
      lines_attacked: # TODO: define which lines fit case5
        - '0_2_1'
        - '0_3_2'
        - '2_3_6'
  seed: *seed
  rho_threshold: 0.9

# multi agent
policy_mapping_fn: !policy_mapping_fn
policies_to_train: ["reinforcement_learning_policy"]

# framework
framework: torch

# rl module
_enable_rl_module_api: False

# exploration
exploration_config:
  type: EpsilonGreedy

# callbacks
callbacks: !CustomMetricsCallback

# paths
storage_path: /Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/ #/home3/s3374610/mahrl_grid2op/runs/ 