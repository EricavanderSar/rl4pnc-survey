# training
nb_timesteps: &nb_timesteps 1000
checkpoint_freq: 100
verbose: 1
gamma: 0.95 # !grid_search [0.95, 0.99]
lr: 0.003 # !grid_search [0.0005, 0.005, 0.05]
vf_loss_coeff: 0.5
entropy_coeff: 0.01
clip_param: 0.2
lambda_: 0.95
sgd_minibatch_size: 32 # !grid_search [32, 64, 128]
train_batch_size: 128
model:
  fcnet_hiddens: 
  - 128
  - 128
_enable_learner_api: False
seed: &seed 16
  
# environment
env_config:
  env_name: rte_case5_example
  num_agents: 3
  action_space: asymmetry
  lib_dir: /Users/ericavandersar/Documents/Python_Projects/Research/mahrl_grid2op/ #/Users/barberademol/Documents/GitHub/mahrl_grid2op/
  max_tsteps: *nb_timesteps
  grid2op_kwargs:
    test: True
    reward_class: !LossReward
  seed: *seed
  rho_threshold: 0.9

# multi agent
policy_mapping_fn: !policy_mapping_fn
policies_to_train: ["reinforcement_learning_policy"]

# framework
framework: torch

# rl module
_enable_rl_module_api: False

# exploration
exploration_config:
  type: EpsilonGreedy

# callbacks
callbacks: !CustomMetricsCallback

# paths
storage_path: ./runs # /Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/