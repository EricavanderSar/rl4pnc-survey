setup:
  max_ep_len: 2016
  nb_timesteps: 5_000
  checkpoint_freq: 2 # number of iterations between checkpoints (1 iteration = 1 agent update?)
  verbose: 0
  storage_path: ./runs #/Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/ #/home3/s3374610/mahrl_grid2op/runs/
  folder_name: "Default params"
  experiment_name: "PPOSingle_Case5_default" # name in WandB
  save_artifact: True
  optimize: False
  num_samples: 10

debugging:
  seed: &seed 14

#checkpointing:
#  checkpoint_trainable_policies_only: True

training:
  checkpoint_trainable_policies_only: True
  # tunable parameters
  lr: 0.003 #!qloguniform [0.0005, 0.05, 0.0001]  # !grid_search [0.0005, 0.005, 0.05]
  gamma: 0.95 # !quniform [0.95, 0.99, 0.01] #
  vf_loss_coeff: 0.5
  entropy_coeff: 0.01
  clip_param: 0.2
  lambda: 0.95
  vf_clip_param: 10
#  use_kl_loss: False # setting this similar to orig EVDS implementation
  num_sgd_iter: 4
  sgd_minibatch_size: 32
  train_batch_size: &train_batch_size 128 # !grid_search [32, 64, 128]
  batch_mode: complete_episodes
  model:
    fcnet_hiddens: [128,128,128] #!grid_search [[256,256], [128,128,128]] #
    fcnet_activation: relu #!choice [relu, tanh]
  _enable_learner_api: False

rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: StochasticSampling

callbacks:
  callbacks: !CustomMetricsCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]
  count_steps_by: agent_steps

environment:
  env_config: &env_config_train
    env_type: new_env # old_env #
    shuffle_scenarios: True
    env_name: rte_case5_example_train
    action_space: tennet
    lib_dir: /Users/ericavandersar/Documents/Python_Projects/Research/mahrl_grid2op/ #/Users/barberademol/Documents/GitHub/mahrl_grid2op/ # /home3/s3374610/mahrl_grid2op/
    max_tsteps: *train_batch_size
    grid2op_kwargs:
      # general kwargs
#      test: True
      reward_class: !LossReward
    seed: *seed
    rho_threshold: 0.90
    n_history: 3
    input: ["p_i", "r", "o", "d"]
    danger: 0.9
    adj_matrix: False #use adjacency matrix instead of topology vector (is adj matrix useful for lin NN??)
    prio: True
    normalize: True

env_config_val: &env_config_val
  <<: *env_config_train
  env_name: rte_case5_example_val
  shuffle_scenarios: False
  prio: False

evaluation:
  evaluation_interval: 1
  evaluation_duration: 2 # as many as there are validation episodes
  evaluation_duration_unit: episodes
  always_attach_evaluation_results: True
  evaluation_config:
    explore: False
    env_config: *env_config_val # use the validation environment

#scaling_config:
#  num_workers: 1

reporting:
  keep_per_episode_custom_metrics: True

resources:
  num_learner_workers: 8

rollouts:
  num_rollout_workers: 3

my_log_level: 1
