setup:
  nb_timesteps: &nb_timesteps 100000
  # checkpoint_freq: 10000
  verbose: 1
  storage_path: /Users/barberademol/Documents/GitHub/mahrl_grid2op/runs/ #/home3/s3374610/mahrl_grid2op/runs/ 
  experiment_name: bugfixing
  save_artifact: True
  num_samples: 1

debugging:
  seed: &seed 14

training:
  # tunable parameters
  lr: 0.00005 #!grid_search [0.00005, 0.0005, 0.005]
  gamma: 0.95 # !grid_search [0.95, 0.99]
  vf_loss_coeff: 0.5
  entropy_coeff: 0.01
  clip_param: 0.2
  lambda_: 0.95
  sgd_minibatch_size: 2000 # !grid_search [32, 64, 128]
  train_batch_size: 40000 # !grid_search [800, 8000, 80000]
  model:
    fcnet_hiddens: 
    - 256
    - 256
  _enable_learner_api: False

rl_module:
  _enable_rl_module_api: False

explore:
  exploration_config:
    type: StochasticSampling

callbacks:
  callbacks: !CustomMetricsCallback

framework:
  framework: torch

multi_agent:
  policy_mapping_fn: !policy_mapping_fn
  policies_to_train: ["reinforcement_learning_policy"]

environment:
  env_config: &env_config_train
    env_name: rte_case5_example
    num_agents: 3
    action_space: asymmetry
    lib_dir: /Users/barberademol/Documents/GitHub/mahrl_grid2op/ # /home3/s3374610/mahrl_grid2op/ 
    max_tsteps: *nb_timesteps
    grid2op_kwargs:
      # general kwargs
      reward_class: !LossReward
    seed: *seed
    rho_threshold: 0.95

env_config_val: &env_config_val
  <<: *env_config_train
  env_name: rte_case5_example

evaluation:
  evaluation_interval: 1000
  evaluation_duration: 14 # as many as there are validation episodes
  evaluation_duration_unit: episodes
  evaluation_config: 
      explore: False
      env_config: *env_config_val # use the validation environment
