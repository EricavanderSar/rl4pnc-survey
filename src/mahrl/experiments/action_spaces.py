"""
Implements the codes to configure the three kinds of action spaces.
"""

import json
import os.path
import time
from collections import Counter
from typing import List
from tqdm import tqdm
from itertools import compress
from functools import partial
from pathos.multiprocessing import ProcessingPool as Pool

import grid2op
import numpy as np
import pandas as pd
from grid2op.Action import BaseAction
from grid2op.Observation import BaseObservation
from grid2op.Environment import BaseEnv, MultiEnvMultiProcess, SingleEnvMultiProcess
from lightsim2grid import LightSimBackend

from mahrl.grid2op_env.utils import get_possible_topologies

def get_changeable_substations_tennet(env: BaseEnv) -> list[int]:
    """
    Find all substations that have more than four lines and can therefore be acted upon
    according to the proposed TenneT action space.
    """
    changeable_substations = []

    # for all substations
    nr_substations = len(env.sub_info)
    for sub in range(nr_substations):
        nr_elements = len(env.observation_space.get_obj_substations(substation_id=sub))
        nr_non_lines = sum(
            1
            for row in env.observation_space.get_obj_substations(substation_id=sub)
            if row[1] != -1 or row[2] != -1
        )
        nr_lines = nr_elements - nr_non_lines

        # append this substation if it has more than four lines
        if nr_lines >= 4:
            changeable_substations.append(sub)

    return changeable_substations


def get_action_space(env: BaseEnv,
                     act_space_name: str,
                     **kwargs
                     ) -> list[BaseAction]:
    """
        This returns all actions as generated by get_possible_topologies().
    """
    incl_dn = kwargs.get("extra_donothing", False)
    rho_filter = kwargs.get("rho_filter", 2.0)
    workers = kwargs.get("workers", 8)
    adjust_shunt = kwargs.get("adjust_shunt")
    greedy_filter = kwargs.get("greedy_filter", False)
    print(f'Initialize "{act_space_name}" action space with dn={incl_dn}...')
    changeable_substations = get_changeable_substations_tennet(env) if act_space_name.startswith('tennet') else list(
        range(len(env.sub_info)))

    # Generic part
    legal_actions = []
    # all substations
    possible_substation_actions = get_possible_topologies(
        env, changeable_substations
    )

    if not incl_dn:
        # Create the DataFrame with defined columns
        dataframe = pd.DataFrame(
            columns=list(
                range(len(possible_substation_actions[0].to_json()["_set_topo_vect"]))
            )
        )

    if act_space_name == 'tennet':
        non_lines = np.concatenate(
            (env.action_space.gen_pos_topo_vect, env.action_space.load_pos_topo_vect)
        ).tolist()

    for action in possible_substation_actions:
        topo_vect = action.set_bus
        if act_space_name == 'medha' or act_space_name == 'tennet':
            if act_space_name =='medha':
                # Select elements for this substation
                sub_elements = [num for num in topo_vect if num != 0]
            if act_space_name == 'tennet':
                # Select line elements for this substation
                lines_topo_vect = np.delete(topo_vect, non_lines)
                # Filter out non-zero elements
                sub_elements = [num for num in lines_topo_vect if num != 0]

            # Count occurrences of each non-zero element
            element_counts = Counter(sub_elements)

            # Check if there are at least two occurrences of each non-zero element
            at_least_two_occurrences = all(count >= 2 for count in element_counts.values())

            if incl_dn:
                if at_least_two_occurrences:
                    legal_actions.append(action)
            else:
                # get_possible_topologies already takes into account that a generator connected to no
                # substation-substation lines does not work, so the constraint to check if one
                # substation-substation line is connected to each busbar does not need to be tested
                if at_least_two_occurrences:
                    dataframe.loc[len(dataframe.index)] = topo_vect
                else:  # add dummy to keep index intact
                    dataframe.loc[len(dataframe.index)] = [0] * env.dim_topo
        else:
            # include all actions, no filtering applied.
            if incl_dn:
                legal_actions.append(action)
            else:
                # Save actions in df such that actions can be filtered later
                dataframe.loc[len(dataframe.index)] = topo_vect

    if incl_dn:
        # add do nothing actions of subs with only 1 line:
        count_lines = Counter(env.line_ex_to_subid) + Counter(env.line_or_to_subid)
        one_line_subs = [sub for sub, n_lines in count_lines.items() if n_lines == 1]
        for sub in one_line_subs:
            # Default DN topology
            topo = np.ones(env.sub_info[sub], dtype=int)
            action = env.action_space(
                {"set_bus": {"substations_id": [(sub, topo)]}}
            )
            legal_actions.append(action)
    else:
        for index, row in dataframe.iterrows():
            # Identify positive entries in the current row
            positive_entries = row[row > 0].index.tolist()

            # Check if the positive entries in the current row have corresponding empty columns in other rows
            # in other words check if there exists another action next to this one (otherwise it is the DN action)
            meets_condition = all(
                dataframe.loc[dataframe.index != index, positive_entries].eq(0).all(axis=1)
            )

            if not meets_condition:
                legal_actions.append(possible_substation_actions[index])

    if adjust_shunt == "opt":
        legal_actions = get_optshunt_actions(env, legal_actions)
    elif adjust_shunt == "all":
        legal_actions = get_allshunt_actions(env, legal_actions)
    if greedy_filter:
        tic = time.time()
        legal_actions = multi_greedy_filter(env,
                                            legal_actions,
                                            rho_filter,
                                            workers,
                                            kwargs.get("act_pool_size", 30),
                                            kwargs.get("nb_interact", 100))
        toc = time.time() - tic
        print(f"Duration greedy pool filter heuristic: {toc:.2f}")
    elif rho_filter < 2.0:
        legal_actions = apply_rho_filter(env, legal_actions, rho_filter, workers)

    return legal_actions


def get_space_numpy(env: BaseEnv,
                    act_space_name: str,
                    path: str,
                    **kwargs) -> list[BaseAction]:
    incl_dn = kwargs.get("extra_donothing", False)
    rho_filter = kwargs.get("rho_filter", 2.0)
    workers = kwargs.get("workers", 8)
    print(f'Initialize "{act_space_name}" action space with dn={incl_dn}...')
    path_data = os.path.join(path, act_space_name)
    files = os.listdir(path_data)
    action_list = []
    for file in files:
        action_arrays = np.load(os.path.join(path_data, file))
        print(action_arrays)
        for action in action_arrays:
            print("complete:  ", action)
            print("shape: ", action.shape)
            g2op_action: BaseAction = env.action_space.from_vect(action)
            # print("set bus: ", g2op_action.set_bus)
            # print("switch bus: ", g2op_action.change_bus)
            # print(g2op_action.as_dict())
            action_list.append(g2op_action)

    if incl_dn:
        for sub in range(env.n_sub):
            # Default DN topology
            topo = np.ones(env.sub_info[sub], dtype=int)
            action = env.action_space(
                {"set_bus": {"substations_id": [(sub, topo)]}}
            )
            action_list.append(action)

    if rho_filter < 2.0:
        action_list = apply_rho_filter(env, action_list, rho_filter, workers)

    return action_list


def get_optshunt_actions(env: BaseEnv, actions: list[BaseAction]) -> list[BaseAction]:
    """
    This function checks for all actions related to a substation with shunt if the reversed action is better
    If this is the case the action is replaced by the reversed action

    Parameters
    ----------
    env: Grid2op Environment
    actions: List of grid2op actions.

    Returns
    -------
    actions: List of grid2op actions, similar to the old list, but  where some actions have been reversed
    """
    print("\nUse best actions for subs with shunt")
    env_rev = grid2op.make(env.env_name)
    count = 0
    for idx, act in enumerate(tqdm(actions)):
        lines_impact, subs_impact = act.get_topological_impact()
        if subs_impact[env.shunt_to_subid].any():
            sub_act = int(np.arange(env.n_sub)[subs_impact])
            topo_act = act.sub_set_bus[act.sub_set_bus > 0]
            # print(f'Substation {sub_act}, Topo {topo_act} ')
            # compute reversed action:
            rev_topo = np.where(topo_act == 1, 2, 1)
            act_rev = env.action_space(
                {"set_bus": {"substations_id": [(sub_act, rev_topo)]}}
            )
            # test n times and save best result.
            n = 20
            rhos_normal = np.zeros(n)
            rhos_rev = np.zeros(n)
            for i in range(n):
                # print("Chronic id of env: ", env.chronics_handler.get_name())
                # print("Chronic id of env_rev: ", env_rev.chronics_handler.get_name())
                obs = env.reset()
                obs_rev = env_rev.reset()
                # try action normal
                obs, reward, done, info = env.step(act)
                # try action reversed
                obs_rev, reward_rev, done_rev, info_rev = env_rev.step(act_rev)
                rhos_normal[i] = obs.rho.max() if obs.rho.max() > 0 else 2
                rhos_rev[i] = obs_rev.rho.max() if obs_rev.rho.max() > 0 else 2
            # # print mean max rho values
            # print('max rho NORMAL: ', rhos_normal.mean())
            # # print('vec: ', rhos_normal)
            # print('max rho REVERSED: ', rhos_rev.mean())
            # # print('vec: ', rhos_rev)
            if rhos_normal.mean() > rhos_rev.mean():
                actions[idx] = act_rev
                count += 1
                # print(f'Action adjusted to reverse topo: {rev_topo}')
                # print(actions[idx])
            # else:
            #     # print('Action remains the same.')
            #     # print(actions[idx])
    print(f"\n{count}/{len(actions)} actions have been reversed.")
    return actions


def get_allshunt_actions(env: BaseEnv, actions: list[BaseAction]) -> list[BaseAction]:
    """
    Include also reversed action to action space for substations with shunt.
    Parameters
    ----------
    env: Grid2op Environment
    actions: List of grid2op actions.

    Returns
    -------
    actions: List of grid2op actions, similar to the old list, but  where actions have been added for substations
    with shunt.
    """
    print("\nUse all actions for subs with shunt.")
    new_actions = []
    for idx, act in enumerate(tqdm(actions, desc="Selecting all shunt actions")):
        lines_impact, subs_impact = act.get_topological_impact()
        if subs_impact[env.shunt_to_subid].any():
            sub_act = int(np.arange(env.n_sub)[subs_impact])
            topo_act = act.sub_set_bus[act.sub_set_bus > 0]
            # print(f'Substation {sub_act}, Topo {topo_act} ')
            # compute reversed action:
            rev_topo = np.where(topo_act == 1, 2, 1)
            act_rev = env.action_space(
                {"set_bus": {"substations_id": [(sub_act, rev_topo)]}}
            )
            new_actions.append(act_rev)
    actions.extend(new_actions)
    print(f"Action space increased with {len(new_actions)} actions")
    return actions


def action_satisfies_rhomax(multi_envs, act, rho_max, nb_workers):
    # Test each action at least 16 times to get mean of max rhos
    n = int(np.ceil(16/nb_workers))
    max_rhos = np.zeros(n * nb_workers)
    for i in range(n):
        for j, env in enumerate(multi_envs.envs):
            # Make sure each env uses different chronic and all actions are tested on the same chronics
            env.set_id(j * 10 * (i+1) + j)
        obss = multi_envs.reset()
        # print("multi_envs chron after reset: ", [e.chronics_handler.get_name() for e in multi_envs.envs])
        # try action normal
        obss, rewards, dones, infos = multi_envs.step([act for _ in range(nb_workers)])
        max_rhos[i*nb_workers : i*nb_workers + nb_workers] = [obs.rho.max() if not done else 2 for obs, done in zip(obss, dones)]
    return np.mean(max_rhos) < rho_max


def apply_rho_filter(env: BaseEnv, actions: list[BaseAction], rho_max: float, nb_workers: int) -> list[BaseAction]:
    """
        This function checks for all actions if the mean rho value is higher than rho_max.
        If this is the case this action will not be included in the new_action_list.

        Parameters
        ----------
        env: Grid2op Environment
        actions: List of grid2op actions.
        rho_max: Float representing the limit of accepted actions.
        nb_workers: number of workers used to eval the actions

        Returns
        -------
        actions: List of grid2op actions, similar to the old list, but  where some actions have been reversed
        """
    print(f"\nUse rho_max {rho_max} to filter action space")
    nb_workers = nb_workers
    envs = []
    for i in range(nb_workers):
        new_env = grid2op.make(env.env_name, backend=LightSimBackend())
        new_env.chronics_handler.set_chunk_size(100)
        envs.append(new_env)

    print(f"Creating a multiprocess environment with {nb_workers} workers...")
    multi_envs = MultiEnvMultiProcess(envs, np.ones(nb_workers))
    # multi_envs = MultiEnvMultiProcess([env], [nb_workers])
    # print("multi_envs chron: ", [e.chronics_handler.get_name() for e in multi_envs.envs])
    new_action_list = []
    for idx, act in enumerate(tqdm(actions, desc="Evaluating all actions")):
        if action_satisfies_rhomax(multi_envs, act, rho_max, nb_workers):
            new_action_list.append(act)
    print(f"{len(new_action_list)}/{len(actions)} actions are kept in the new action space. "
          f"\nPercentage: {len(new_action_list)/len(actions) * 100:.2f}%")
    return new_action_list


class ActionPoolAgent:
    def __init__(self, action_pool: list[BaseAction], env: BaseEnv, rho_max: float):
        self.action_space = action_pool
        self.do_nothing = [env.action_space() for _ in action_pool]
        self.good_actions = np.zeros(len(self.action_space))
        self.rho_max = rho_max

    def act(self, obss: list[BaseObservation]):
        print("rho max per environment ", [obs.rho.max()  for obs in obss])
        max_obs = np.max([obs.rho.max() for obs in obss])
        if max_obs > 0.95: # if for any of the envs the max_rho>0.95
            return self.action_space, True
        else:
            return self.do_nothing, False

    def evaluate_act(self, obss: list[BaseObservation], dones):
        rho_maxs = np.array([obs.rho.max() if not done else 2 for obs, done in zip(obss, dones)])
        print("rho_maxs: ", rho_maxs)
        self.good_actions = (rho_maxs < self.rho_max)
        print("good actions: ", self.good_actions)

    def return_acts_to_keep(self) -> list[BaseAction]:
        return list(compress(self.action_space, self.good_actions))


def create_action_pools(action_list: list[BaseAction], pool_size: int):
    pools = []
    for i in range(0, len(action_list), pool_size):
        pool = action_list[i:i + pool_size]
        if len(pool) < pool_size:
            pool = action_list[-pool_size:]
        pools.append(pool)
    return pools


def filter_actions(env: BaseEnv, actions: list[BaseAction], rho_max: float, nb_workers: int) -> list[BaseAction]:
    print(f"Creating a {np.ceil(len(actions)/nb_workers)} action pools...")
    pools = create_action_pools(actions, nb_workers)
    print(f"{len(pools)} action pools created.")

    print(f"Creating a multiprocess environment with {nb_workers} workers...")
    multi_envs = SingleEnvMultiProcess(env, nb_workers)
    nb_eval = 1 # how often will each action pool be evaluated
    filtered_actions = []
    for action_pool in pools:
        agent = ActionPoolAgent(action_pool, env, rho_max)
        for i in range(nb_eval):
            multi_envs.set_id(i)
            obss = multi_envs.reset()
            print("multi_envs chron after reset: ", [e.chronics_handler.get_name() for e in multi_envs.envs])
            dones = [False]
            while not any(dones):
                actions, acted = agent.act(obss)
                obss, rewards, dones, infos = multi_envs.step(actions=actions)
                if acted:
                    print(dones)
                    agent.evaluate_act(obss, dones)
        filtered_actions.extend(agent.return_acts_to_keep())
        # print(f"filtered_actions is now of lenght: {len(filtered_actions)}")
    multi_envs.close()
    return filtered_actions


class GreedyAgentFilter:
    def __init__(self, action_pool: list[BaseAction], env: BaseEnv, rho_max: float):
        self.action_space = action_pool
        self.do_nothing = env.action_space()
        self.act_count = np.zeros(len(self.action_space))
        self.bad_act_count = np.zeros(len(self.action_space))
        self.bad_threshold = 3
        self.rho_max = rho_max

    def act(self, obs: BaseObservation):
        if obs.rho.max() > 0.95: # if for any of the envs the max_rho>0.95
            # print(f"Current action_space is of size: {len(self.action_space)}")
            max_rhos = np.zeros(len(self.action_space))
            for idx, act in enumerate(self.action_space):
                sim_obs, sim_reward, sim_done, sim_info = obs.simulate(act)
                max_rhos[idx] = sim_obs.rho.max() if not sim_done else 2
            best_act = self.action_space[np.argmin(max_rhos)]
            self.act_count[np.argmin(max_rhos)] += 1
            self.bad_act_count[(max_rhos > max(min(max_rhos)+0.1, self.rho_max))] += 1
            if max(self.bad_act_count) > self.bad_threshold:
                self._filter_action_pool()
            return best_act, True
        else:
            return self.do_nothing, False

    def _filter_action_pool(self):
        # print(f"Current action_space is of size: {len(self.action_space)}")
        # print(f"Actions that so far have been selected: {self.act_count}")
        # print(f"Bad action count: ", self.bad_act_count)
        corr_bad_count = self.bad_act_count - self.act_count
        self.action_space = list(compress(self.action_space, corr_bad_count <= self.bad_threshold))
        self.act_count = self.act_count[corr_bad_count <= self.bad_threshold]
        self.bad_act_count = self.bad_act_count[corr_bad_count <= self.bad_threshold]
        # print(f"New action_space is of size: {len(self.action_space)}")

    def return_acts_to_keep(self) -> (list[BaseAction], list[BaseAction]):
        return list(compress(self.action_space, self.act_count>1)), self.action_space


def apply_greedy_filter(actions: list[BaseAction],
                        input_env: BaseEnv,
                        rho_max: float,
                        nb_interactions: int
                        ) -> (list[BaseAction], list[BaseAction]):
    # print("Creating copy of environment...")
    env = grid2op.make(input_env.env_name, backend=LightSimBackend())
    env.seed(0)
    env.chronics_handler.set_chunk_size(100)
    env.generate_classes()
    # print("Creating GreedyAgentFilter...")
    agent = GreedyAgentFilter(actions, env, rho_max)
    i = 0
    pbar = tqdm(total = nb_interactions)
    while i < nb_interactions:
        obs = env.reset()
        done = False
        while not done and (i < nb_interactions):
            action, acted = agent.act(obs)
            obs, rw, done, info = env.step(action)
            if acted:
                i += 1
                pbar.update(1)
    env.close()
    pbar.close()
    return agent.return_acts_to_keep()


def multi_greedy_filter(env: BaseEnv,
                        actions: list[BaseAction],
                        rho_max: float,
                        nb_workers: int,
                        pool_size: int = 30,
                        nb_interact: int = 100) -> (list[BaseAction], list[BaseAction]):
    print(f"Creating  action pools of size {pool_size}...")
    pools = create_action_pools(actions, pool_size)
    print(f"{len(pools)} action pools created.")

    greedy_actions_list = []
    rho_filtered_list = []
    env.generate_classes()
    with Pool(nb_workers) as pool:
        worker = partial(apply_greedy_filter, input_env=env, rho_max=rho_max, nb_interactions=nb_interact)
        results = pool.map(worker, pools)

        for res in results:
            greedy_actions, rho_filtered_actions = res
            greedy_actions_list.extend(greedy_actions)
            rho_filtered_list.extend(rho_filtered_actions)
    print(f"The greedy action_space size is {len(greedy_actions_list)}")
    print(f"The rho filtered action_space size is {len(rho_filtered_list)}")
    return greedy_actions_list, rho_filtered_list

    # print(f"Creating a multiprocess environment with {nb_workers} workers...")
    # multi_envs = SingleEnvMultiProcess(env, nb_workers)
    # agents = [GreedyAgentFilter(action_pool, env, rho_max) for action_pool in pools]
    #
    # nb_interactions = 10
    # i = 0
    # obs = multi_envs.reset()
    # while i < nb_interactions:
    #     acts = [None for _ in range(nb_workers)]
    #     acted = np.zeros(nb_workers)
    #     for env_act_id in range(nb_workers):
    #         acts[env_act_id], acted[env_act_id] = agents[env_act_id].act(obs[env_act_id])
    #     if any(acted):
    #         i += 1
    #     obs, rews, dones, infos = multi_envs.step(acts)
    #
    # filtered_actions = []
    # for agent in agents:
    #     filtered_actions.extend(agent.return_acts_to_keep())
    #
    # print(f"Filtered_actions is now of lenght: {len(filtered_actions)}")
    # multi_envs.close()
    # return filtered_actions


def save_to_json(
    possible_substation_actions: List[BaseAction], json_file_path: str
) -> None:
    """Saves list of actions to .json that can be used in training."""
    actions_to_json = [
        {
            "set_bus": {
                "loads_id": [
                    [int(elem_id), int(bus_id)]
                    for elem_id, bus_id in enumerate(action.load_set_bus)
                    if bus_id > 0
                ],
                "generators_id": [
                    [int(elem_id), int(bus_id)]
                    for elem_id, bus_id in enumerate(action.gen_set_bus)
                    if bus_id > 0
                ],
                "lines_or_id": [
                    [int(elem_id), int(bus_id)]
                    for elem_id, bus_id in enumerate(action.line_or_set_bus)
                    if bus_id > 0
                ],
                "lines_ex_id": [
                    [int(elem_id), int(bus_id)]
                    for elem_id, bus_id in enumerate(action.line_ex_set_bus)
                    if bus_id > 0
                ],
            }
        }
        if any(action.set_bus) else
        {
            "change_bus": {
                "loads_id": np.where(action.load_change_bus)[0].tolist(),
                "generators_id": np.where(action.gen_change_bus)[0].tolist(),
                "lines_or_id": np.where(action.line_or_change_bus)[0].tolist(),
                "lines_ex_id": np.where(action.line_ex_change_bus)[0].tolist(),
            }
        }
        for action in possible_substation_actions
    ]
    print(actions_to_json)
    # Save to json
    with open(json_file_path, "wt", encoding="utf-8") as file:
        json.dump(actions_to_json, file)


# if __name__ == "__main__":
#     # envs = ["rte_case5_example", "rte_case14_realistic", "l2rpn_wcci_2022"]
#     envs = ["rte_case5_example", "rte_case14_realistic"]
#     for env_name in envs:
#         environment = grid2op.make(env_name, test=True)
#
#         file_path = f"/Users/barberademol/Documents/GitHub/mahrl_grid2op/experiments/action_spaces/{env_name}/asymmetry.json"
#         save_to_json(get_asymmetrical_action_space(environment), file_path)
#
#         file_path = f"/Users/barberademol/Documents/GitHub/mahrl_grid2op/experiments/action_spaces/{env_name}/medha.json"
#         save_to_json(get_medha_action_space(environment), file_path)
#
#         file_path = f"/Users/barberademol/Documents/GitHub/mahrl_grid2op/experiments/action_spaces/{env_name}/tennet.json"
#         save_to_json(get_tennet_action_space(environment), file_path)
