{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-03 09:48:38,930\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-11-03 09:48:39,803\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-11-03 09:48:40,616\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import grid2op\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "from grid2op.gym_compat import GymEnv, ScalerAttrConverter, MultiToTupleConverter\n",
    "from ray.rllib.algorithms import ppo  # import the type of agents\n",
    "from typing import Any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"rte_case5_example\"\n",
    "LIBRARY_DIRECTORY = \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/grid2op/data/\"\n",
    "NB_STEP_TRAIN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/grid2op/data/rte_case5_example_train\n"
     ]
    }
   ],
   "source": [
    "print(LIBRARY_DIRECTORY + ENV_NAME + \"_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run first time to set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LIBRARY_DIRECTORY + ENV_NAME + \"_train\"):\n",
    "    env = grid2op.make(ENV_NAME, test=True)\n",
    "\n",
    "    # extract 10% of the \"chronics\" to be used in the validation environment, 10% for testing,\n",
    "    # 80% for training\n",
    "    nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(\n",
    "        pct_val=10.0, pct_test=10.0, add_for_test=\"test\"\n",
    "    )\n",
    "    # and now you can use the training set only to train your agent:\n",
    "    print(f\"The name of the training environment is {nm_env_train}\")\n",
    "    print(f\"The name of the validation environment is {nm_env_val}\")\n",
    "    print(f\"The name of the test environment is {nm_env_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The grid2op documentation is full of details to \"optimize\" the number of steps you can do\n",
    "# per seconds. This number can rise from a few dozen per seconds to around a thousands per seconds\n",
    "# with proper care. We strongly encouraged you to leverage all the possibilities which includes\n",
    "# (but are not limited to):\n",
    "# - using \"lightsim2grid\" as a backend for a 10-15x speed up in the \"env.step(...)\" function\n",
    "# - using \"MultifolderWithCache\"/\"env.chronics_handler.set_chunk(...)\" for faster \"env.reset(...)\"\n",
    "#   see https://grid2op.readthedocs.io/en/latest/environment.html#optimize-the-data-pipeline\n",
    "# - using \"SingleEnvMultiProcess\" for parrallel computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyEnv class, and train a Proximal Policy Optimisation based agent\n",
    "class MyEnv(gym.Env):\n",
    "    \"\"\"Encapsulate Grid2Op environment and set action/observation space.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        # 1. create the grid2op environment\n",
    "        if not \"env_name\" in env_config:\n",
    "            raise RuntimeError(\n",
    "                \"The configuration for RLLIB should provide the env name\"\n",
    "            )\n",
    "        nm_env = env_config[\"env_name\"]\n",
    "        del env_config[\"env_name\"]\n",
    "        self.env_glop = grid2op.make(nm_env, **env_config)\n",
    "\n",
    "        # 2. create the gym environment\n",
    "        self.env_gym = GymEnv(self.env_glop)\n",
    "        obs_gym = self.env_gym.reset()\n",
    "\n",
    "        # 3. (optional) customize it (see section above for more information)\n",
    "        # customize action space\n",
    "        self.env_gym.action_space = self.env_gym.action_space.ignore_attr(\n",
    "            \"set_bus\"\n",
    "        ).ignore_attr(\"set_line_status\")\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\n",
    "            \"change_bus\", MultiToTupleConverter()\n",
    "        )\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\n",
    "            \"change_line_status\", MultiToTupleConverter()\n",
    "        )\n",
    "        ## customize observation space\n",
    "        ob_space = self.env_gym.observation_space\n",
    "        ob_space = ob_space.keep_only_attr(\n",
    "            [\"rho\", \"gen_p\", \"load_p\", \"topo_vect\", \"actual_dispatch\"]\n",
    "        )\n",
    "        ob_space = ob_space.reencode_space(\n",
    "            \"actual_dispatch\",\n",
    "            ScalerAttrConverter(substract=0.0, divide=self.env_glop.gen_pmax),\n",
    "        )\n",
    "        ob_space = ob_space.reencode_space(\n",
    "            \"gen_p\", ScalerAttrConverter(substract=0.0, divide=self.env_glop.gen_pmax)\n",
    "        )\n",
    "        ob_space = ob_space.reencode_space(\n",
    "            \"load_p\",\n",
    "            ScalerAttrConverter(\n",
    "                substract=obs_gym[0][\"load_p\"], divide=0.5 * obs_gym[0][\"load_p\"]\n",
    "            ),\n",
    "        )\n",
    "        self.env_gym.observation_space = ob_space\n",
    "\n",
    "        # 4. specific to rllib\n",
    "        self.action_space = self.env_gym.action_space\n",
    "        self.observation_space = self.env_gym.observation_space\n",
    "\n",
    "        # 4.to avoid other type of issues, we recommend to build the action space and observation\n",
    "        # space directly from the spaces class.\n",
    "        d = {k: v for k, v in self.env_gym.observation_space.spaces.items()}\n",
    "        self.observation_space = gym.spaces.Dict(d)\n",
    "        a = {k: v for k, v in self.env_gym.action_space.items()}\n",
    "        self.action_space = gym.spaces.Dict(a)\n",
    "\n",
    "    def reset(self, seed: int = None, options: dict[str, Any] = None):\n",
    "        obs = self.env_gym.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        print(self.env_gym.step(action))\n",
    "        obs, reward, done, info = self.env_gym.step(action)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 10:06:26,482\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-11-03 10:06:26,483\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='<class '__main__.MyEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class '__main__.MyEnv'>').build()` instead. This will raise an error in the future!\n",
      "/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-11-03 10:06:28,083\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=11014)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11014)\u001b[0m /Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/grid2op/MakeEnv/Make.py:433: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11014)\u001b[0m   warnings.warn(_MAKE_DEV_ENV_WARN)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11013)\u001b[0m 2023-11-03 10:06:32,283\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-11-03 10:06:32,344\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n",
      "  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\n",
      "ValueError: too many values to unpack (expected 4)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 404, in __init__\n",
      "    check_env(self.env, self.config)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n",
      "    raise ValueError(\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 234, in check_gym_environments\n",
      "    results = env.step(sampled_action)\n",
      "  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\n",
      "ValueError: too many values to unpack (expected 4)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "2023-11-03 10:06:32,345\tERROR actor_manager.py:500 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11014, ip=127.0.0.1, actor_id=cd54246541d1d4eeb1ce6f9b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1459523b0>)\n",
      "  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\n",
      "ValueError: too many values to unpack (expected 4)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11014, ip=127.0.0.1, actor_id=cd54246541d1d4eeb1ce6f9b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1459523b0>)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11014, ip=127.0.0.1, actor_id=cd54246541d1d4eeb1ce6f9b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1459523b0>)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 404, in __init__\n",
      "    check_env(self.env, self.config)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n",
      "    raise ValueError(\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 234, in check_gym_environments\n",
      "    results = env.step(sampled_action)\n",
      "  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\n",
      "ValueError: too many values to unpack (expected 4)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11014, ip=127.0.0.1, actor_id=cd54246541d1d4eeb1ce6f9b01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x1459523b0>)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n",
      "    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n",
      "  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n",
      "    raise ValueError(\n",
      "ValueError: Your gymnasium.Env's `step()` method raised an Exception!\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "\u001b[2m\u001b[36m(pid=11013)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11013)\u001b[0m /Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/grid2op/MakeEnv/Make.py:433: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11013)\u001b[0m   warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 234, in check_gym_environments\n    results = env.step(sampled_action)\n  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\nValueError: too many values to unpack (expected 4)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n    raise ValueError(\nValueError: Your gymnasium.Env's `step()` method raised an Exception!\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:157\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup(\n\u001b[1;32m    158\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    159\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    160\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    161\u001b[0m         local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:227\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_workers(\n\u001b[1;32m    228\u001b[0m     num_workers,\n\u001b[1;32m    229\u001b[0m     validate\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvalidate_workers_after_construction,\n\u001b[1;32m    230\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:593\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39mok:\n\u001b[0;32m--> 593\u001b[0m     \u001b[39mraise\u001b[39;00m result\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:481\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 481\u001b[0m     result \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(r)\n\u001b[1;32m    482\u001b[0m     remote_results\u001b[39m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[39m=\u001b[39mresult), tag)\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/_private/worker.py:2549\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2548\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2549\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   2551\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\nValueError: too many values to unpack (expected 4)\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n    raise ValueError(\nValueError: Your gymnasium.Env's `step()` method raised an Exception!\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 404, in __init__\n    check_env(self.env, self.config)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 96, in check_env\n    raise ValueError(\nValueError: Traceback (most recent call last):\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 234, in check_gym_environments\n    results = env.step(sampled_action)\n  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\nValueError: too many values to unpack (expected 4)\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11013, ip=127.0.0.1, actor_id=c5212b79616fb498d714504401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x124dbe590>)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n    raise ValueError(\nValueError: Your gymnasium.Env's `step()` method raised an Exception!\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m NB_STEP_TRAIN:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m# then define a \"trainer\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         trainer \u001b[39m=\u001b[39m ppo\u001b[39m.\u001b[39;49mPPO(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             env\u001b[39m=\u001b[39;49mMyEnv,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             config\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39menv_config\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39menv_name\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mrte_case5_example_train\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                 },  \u001b[39m# config to pass to env class\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# and then train it for a given number of iteration\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/getting_started.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NB_STEP_TRAIN):\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:517\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    504\u001b[0m     \u001b[39m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     },\n\u001b[1;32m    515\u001b[0m }\n\u001b[0;32m--> 517\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    518\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    519\u001b[0m     logger_creator\u001b[39m=\u001b[39;49mlogger_creator,\n\u001b[1;32m    520\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    523\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:185\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_config, storage)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39massert\u001b[39;00m storage\u001b[39m.\u001b[39mtrial_fs_path\n\u001b[1;32m    183\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStorageContext on the TRAINABLE:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mstorage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    186\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[1;32m    187\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:639\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    638\u001b[0m     \u001b[39m# Create a set of env runner actors via a WorkerSet.\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    640\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    641\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    642\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    643\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    644\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[1;32m    645\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    646\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    649\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    652\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:179\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    167\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[1;32m    172\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    173\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    178\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[1;32m    180\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 234, in check_gym_environments\n    results = env.step(sampled_action)\n  File \"/var/folders/9t/p1jgc3cd7v70n4nh294j6kz00000gn/T/ipykernel_9885/1020910598.py\", line 67, in step\nValueError: too many values to unpack (expected 4)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 85, in check_env\n    check_gym_environments(env, AlgorithmConfig() if config is None else config)\n  File \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 237, in check_gym_environments\n    raise ValueError(\nValueError: Your gymnasium.Env's `step()` method raised an Exception!\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env])."
     ]
    }
   ],
   "source": [
    "if NB_STEP_TRAIN:\n",
    "    try:\n",
    "        # then define a \"trainer\"\n",
    "        trainer = ppo.PPO(\n",
    "            env=MyEnv,\n",
    "            config={\n",
    "                \"env_config\": {\n",
    "                    \"env_name\": \"rte_case5_example_train\",\n",
    "                    \"test\": True,\n",
    "                },  # config to pass to env class\n",
    "            },\n",
    "        )\n",
    "        # and then train it for a given number of iteration\n",
    "        for step in range(NB_STEP_TRAIN):\n",
    "            trainer.train()\n",
    "    finally:\n",
    "        # shutdown ray\n",
    "        ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mahrl",
   "language": "python",
   "name": "venv_mahrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
