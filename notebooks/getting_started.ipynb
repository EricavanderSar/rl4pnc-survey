{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import grid2op\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "from grid2op.gym_compat import GymEnv, ScalerAttrConverter, MultiToTupleConverter\n",
    "from ray.rllib.algorithms import ppo  # import the type of agents\n",
    "from ray import tune\n",
    "from typing import Any\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"rte_case5_example\"\n",
    "LIBRARY_DIRECTORY = \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/grid2op/data/\"\n",
    "NB_STEP_TRAIN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run first time to set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the training environment is rte_case5_example_train\n",
      "The name of the validation environment is rte_case5_example_val\n",
      "The name of the test environment is rte_case5_example_test\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(LIBRARY_DIRECTORY + ENV_NAME + \"_train\"):\n",
    "    # env = grid2op.make(ENV_NAME, test=True)\n",
    "    env = grid2op.make(LIBRARY_DIRECTORY + ENV_NAME)\n",
    "\n",
    "    # extract 10% of the \"chronics\" to be used in the validation environment, 10% for testing,\n",
    "    # 80% for training\n",
    "    nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(\n",
    "        pct_val=10.0, pct_test=10.0, add_for_test=\"test\"\n",
    "    )\n",
    "    # and now you can use the training set only to train your agent:\n",
    "    print(f\"The name of the training environment is {nm_env_train}\")\n",
    "    print(f\"The name of the validation environment is {nm_env_val}\")\n",
    "    print(f\"The name of the test environment is {nm_env_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The grid2op documentation is full of details to \"optimize\" the number of steps you can do\n",
    "# per seconds. This number can rise from a few dozen per seconds to around a thousands per seconds\n",
    "# with proper care. We strongly encouraged you to leverage all the possibilities which includes\n",
    "# (but are not limited to):\n",
    "# - using \"lightsim2grid\" as a backend for a 10-15x speed up in the \"env.step(...)\" function\n",
    "# - using \"MultifolderWithCache\"/\"env.chronics_handler.set_chunk(...)\" for faster \"env.reset(...)\"\n",
    "#   see https://grid2op.readthedocs.io/en/latest/environment.html#optimize-the-data-pipeline\n",
    "# - using \"SingleEnvMultiProcess\" for parrallel computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyEnv class, and train a Proximal Policy Optimisation based agent\n",
    "class MyEnv(gym.Env):\n",
    "    \"\"\"Encapsulate Grid2Op environment and set action/observation space.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        # 1. create the grid2op environment\n",
    "        if not \"env_name\" in env_config:\n",
    "            raise RuntimeError(\n",
    "                \"The configuration for RLLIB should provide the env name\"\n",
    "            )\n",
    "        nm_env:str = env_config[\"env_name\"]\n",
    "        del env_config[\"env_name\"]\n",
    "        self.env_glop = grid2op.make(nm_env, **env_config)\n",
    "\n",
    "        # 2. create the gym environment\n",
    "        self.env_gym = GymEnv(self.env_glop)\n",
    "        obs_gym = self.env_gym.reset()\n",
    "\n",
    "        # 3. (optional) customize it (see section above for more information)\n",
    "        # customize action space\n",
    "        self.env_gym.action_space = self.env_gym.action_space.ignore_attr(\n",
    "            \"set_bus\"\n",
    "        ).ignore_attr(\"set_line_status\")\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\n",
    "            \"change_bus\", MultiToTupleConverter()\n",
    "        )\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\n",
    "            \"change_line_status\", MultiToTupleConverter()\n",
    "        )\n",
    "        ## customize observation space\n",
    "        ob_space = self.env_gym.observation_space\n",
    "        ob_space = ob_space.keep_only_attr(\n",
    "            [\"rho\", \"gen_p\", \"load_p\", \"topo_vect\", \"actual_dispatch\"]\n",
    "        )\n",
    "        ob_space = ob_space.reencode_space(\n",
    "            \"actual_dispatch\",\n",
    "            ScalerAttrConverter(substract=0.0, divide=self.env_glop.gen_pmax),\n",
    "        )\n",
    "        ob_space = ob_space.reencode_space(\n",
    "            \"gen_p\", ScalerAttrConverter(substract=0.0, divide=self.env_glop.gen_pmax)\n",
    "        )\n",
    "        ob_space = ob_space.reencode_space(\n",
    "            \"load_p\",\n",
    "            ScalerAttrConverter(\n",
    "                substract=obs_gym[0][\"load_p\"], divide=0.5 * obs_gym[0][\"load_p\"]\n",
    "            ),\n",
    "        )\n",
    "        self.env_gym.observation_space = ob_space\n",
    "\n",
    "        # 4. specific to rllib\n",
    "        self.action_space = self.env_gym.action_space\n",
    "        self.observation_space = self.env_gym.observation_space\n",
    "\n",
    "        # 4.to avoid other type of issues, we recommend to build the action space and observation\n",
    "        # space directly from the spaces class.\n",
    "        d = {k: v for k, v in self.env_gym.observation_space.spaces.items()}\n",
    "        self.observation_space = gym.spaces.Dict(d)\n",
    "        a = {k: v for k, v in self.env_gym.action_space.items()}\n",
    "        self.action_space = gym.spaces.Dict(a)\n",
    "\n",
    "    def reset(self, seed: int = None, options: dict[str, Any] = None):\n",
    "        obs = self.env_gym.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(self.env_gym.step(action))\n",
    "        obs, reward, done, truncated, info = self.env_gym.step(action)\n",
    "        return obs, reward, done, truncated, info\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-11-06 11:44:30</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:08.86        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.2/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 5.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MyEnv_4df63_00000</td><td>TERMINATED</td><td>127.0.0.1:22953</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          62.149</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 2447.95</td><td style=\"text-align: right;\">             12901.5</td><td style=\"text-align: right;\">             472.923</td><td style=\"text-align: right;\">           430.083</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 11:43:21,519\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=22953)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m 2023-11-06 11:43:23,852\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m 2023-11-06 11:43:23,852\tWARNING algorithm_config.py:672 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22957)\u001b[0m OrderedDict([('change_bus', (0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1)), ('change_line_status', (1, 0, 1, 0, 0, 0, 0, 1))])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22957)\u001b[0m (OrderedDict([('actual_dispatch', array([0., 0.], dtype=float32)), ('gen_p', array([0.71999997, 0.58678216], dtype=float32)), ('load_p', array([ 0.02531643, -0.1975309 ,  0.02439034], dtype=float32)), ('rho', array([0.45425078, 0.54077774, 0.4275628 , 0.39348567, 0.46876583,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22957)\u001b[0m        0.23648427, 0.23648427, 0.36574695], dtype=float32)), ('topo_vect', array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22957)\u001b[0m       dtype=int32))]), 6.699284076690674, False, False, {'disc_lines': array([-1, -1, -1, -1, -1, -1, -1, -1], dtype=int32), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'reason_alarm_illegal': None, 'reason_alert_illegal': None, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('More than 1 line status affected by the action: [0 2 7]')], 'rewards': {}})\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22956)\u001b[0m (OrderedDict([('actual_dispatch', array([0., 0.], dtype=float32)), ('gen_p', array([0.71999997, 0.58678216], dtype=float32)), ('load_p', array([-0.22222222, -0.15189871,  0.15584426], dtype=float32)), ('rho', array([0.45425078, 0.54077774, 0.4275628 , 0.39348567, 0.46876583,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22955)\u001b[0m (OrderedDict([('actual_dispatch', array([0., 0.], dtype=float32)), ('gen_p', array([0.13      , 0.89126503], dtype=float32)), ('load_p', array([-0.17777783,  0.12658216,  0.15584426], dtype=float32)), ('rho', array([0.74528545, 0.91429406, 0.7398177 , 0.59687674, 0.7439081 ,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22955)\u001b[0m        0.3728459 , 0.3728459 , 0.5821862 ], dtype=float32)), ('topo_vect', array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m (OrderedDict([('actual_dispatch', array([0., 0.], dtype=float32)), ('gen_p', array([0.14      , 0.82419103], dtype=float32)), ('load_p', array([-0.09523801, -0.10126585, -0.09638556], dtype=float32)), ('rho', array([0.6937646 , 0.8459465 , 0.67958593, 0.5482594 , 0.69653356,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m 2023-11-06 11:43:28,000\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m 2023-11-06 11:43:28,062\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m 2023-11-06 11:43:28,062\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m 2023-11-06 11:43:28,062\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m 2023-11-06 11:43:28,062\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m Install gputil for GPU system monitoring.\n",
      "2023-11-06 11:44:30,763\tINFO tune.py:1143 -- Total run time: 69.25 seconds (68.85 seconds for the tuning loop).\n",
      "\u001b[2m\u001b[36m(pid=22955)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m 2023-11-06 11:43:28,093\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m 2023-11-06 11:43:28,112\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m 2023-11-06 11:43:28,112\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m 2023-11-06 11:43:28,112\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=22953)\u001b[0m 2023-11-06 11:43:28,112\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m OrderedDict([('change_bus', (1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1)), ('change_line_status', (0, 0, 0, 0, 0, 1, 0, 1))])\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m        0.34983304, 0.34983304, 0.54493695], dtype=float32)), ('topo_vect', array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22954)\u001b[0m       dtype=int32))]), 5.016216278076172, False, False, {'disc_lines': array([-1, -1, -1, -1, -1, -1, -1, -1], dtype=int32), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'reason_alarm_illegal': None, 'reason_alert_illegal': None, 'opponent_attack_line': None, 'opponent_attack_sub': None, 'opponent_attack_duration': 0, 'exception': [Grid2OpException IllegalAction IllegalAction('More than 1 line status affected by the action: [5 7]')], 'rewards': {}})\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"num_workers\": 4,\n",
    "    \"num_envs_per_worker\": 1,\n",
    "    \"env\": MyEnv,\n",
    "    \"env_config\": {\n",
    "        \"env_name\": LIBRARY_DIRECTORY + ENV_NAME + \"_train\"},\n",
    "\n",
    "    \"framework\": \"torch\",\n",
    "    # \"model\": {\n",
    "    #     \"vf_share_layers\": True,\n",
    "    # },\n",
    "    \"lr\": 0.0005,\n",
    "    \"gamma\": 0.99,\n",
    "}\n",
    "\n",
    "\n",
    "if NB_STEP_TRAIN:\n",
    "    try:\n",
    "        analysis = tune.run(\n",
    "            ppo.PPO,\n",
    "            config=config,\n",
    "            stop={\"timesteps_total\": 10000},  # Adjust the stopping criterion\n",
    "            verbose=1,\n",
    "            local_dir=\"/Users/barberademol/Documents/GitHub/mahrl_grid2op/notebooks/results\"\n",
    "        )\n",
    "    finally:\n",
    "        # shutdown ray\n",
    "        ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO_MyEnv_4df63_00000]\n"
     ]
    }
   ],
   "source": [
    "print(analysis.trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mahrl",
   "language": "python",
   "name": "venv_mahrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
