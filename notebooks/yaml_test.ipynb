{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 18:42:52,190\tWARNING algorithm_config.py:2604 -- config._enable_rl_module_api was set to False, but no prior exploration config was found to be restored.\n",
      "2023-12-14 18:42:52,191\tWARNING algorithm_config.py:2604 -- config._enable_rl_module_api was set to False, but no prior exploration config was found to be restored.\n",
      "2023-12-14 18:42:52,191\tWARNING algorithm_config.py:2604 -- config._enable_rl_module_api was set to False, but no prior exploration config was found to be restored.\n",
      "2023-12-14 18:42:52,192\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-12-14 18:42:52,192\tWARNING algorithm_config.py:2592 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trains PPO baseline agent.\n",
    "\"\"\"\n",
    "from typing import Any\n",
    "\n",
    "import ray\n",
    "from gymnasium.spaces import Discrete\n",
    "from ray import air, tune\n",
    "from ray.rllib.algorithms import ppo  # import the type of agents\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "from mahrl.experiments.callback import CustomMetricsCallback\n",
    "from mahrl.experiments.rewards import LossReward\n",
    "from mahrl.grid2op_env.custom_environment import CustomizedGrid2OpEnvironment\n",
    "from mahrl.multi_agent.policy import (\n",
    "    DoNothingPolicy,\n",
    "    SelectAgentPolicy,\n",
    "    policy_mapping_fn,\n",
    ")\n",
    "import yaml\n",
    "\n",
    "ENV_NAME = \"rte_case5_example\"\n",
    "ENV_IS_TEST = True\n",
    "LIB_DIR = \"/Users/barberademol/Documents/GitHub/mahrl_grid2op/\"\n",
    "# LIB_DIR = \"/home/daddabarba/VirtualEnvs/mahrl/lib/python3.10/site-packages/grid2op/data\"\n",
    "RHO_THRESHOLD = 0.9\n",
    "NB_TSTEPS = 50000\n",
    "CHECKPOINT_FREQ = 1000\n",
    "VERBOSE = 1\n",
    "\n",
    "policies = {\n",
    "    \"high_level_policy\": PolicySpec(  # chooses RL or do-nothing agent\n",
    "        policy_class=SelectAgentPolicy,\n",
    "        observation_space=None,  # infer automatically from env\n",
    "        action_space=Discrete(2),  # choose one of agents\n",
    "        config=(\n",
    "            AlgorithmConfig()\n",
    "            .training(\n",
    "                _enable_learner_api=False,\n",
    "            )\n",
    "            .rl_module(_enable_rl_module_api=False)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"type\": \"EpsilonGreedy\",\n",
    "                }\n",
    "            )\n",
    "            .rollouts(preprocessor_pref=None)\n",
    "        ),\n",
    "    ),\n",
    "    \"reinforcement_learning_policy\": PolicySpec(  # performs RL topology\n",
    "        policy_class=None,  # use default policy of PPO\n",
    "        observation_space=None,  # infer automatically from env\n",
    "        action_space=None,  # infer automatically from env\n",
    "        config=(\n",
    "            AlgorithmConfig()\n",
    "            .training(\n",
    "                _enable_learner_api=False,\n",
    "            )\n",
    "            .rl_module(_enable_rl_module_api=False)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"type\": \"EpsilonGreedy\",\n",
    "                }\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    \"do_nothing_policy\": PolicySpec(  # performs do-nothing action\n",
    "        policy_class=DoNothingPolicy,\n",
    "        observation_space=None,  # infer automatically from env --TODO not actually needed\n",
    "        action_space=Discrete(1),  # only perform do-nothing\n",
    "        config=(\n",
    "            AlgorithmConfig()\n",
    "            .training(_enable_learner_api=False)\n",
    "            .rl_module(_enable_rl_module_api=False)\n",
    "            .exploration(\n",
    "                exploration_config={\n",
    "                    \"type\": \"EpsilonGreedy\",\n",
    "                }\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "}\n",
    "\n",
    "ppo_config = ppo.PPOConfig()\n",
    "ppo_config = ppo_config.training(\n",
    "    _enable_learner_api=False,\n",
    "    gamma=0.99,\n",
    "    lr=0.00005,\n",
    "    # gamma=tune.grid_search([0.9, 0.99, 0.999]),\n",
    "    # lr=tune.grid_search([0.0003, 0.003, 0.03]),\n",
    "    vf_loss_coeff=0.5,\n",
    "    entropy_coeff=0.01,\n",
    "    clip_param=0.2,\n",
    "    lambda_=0.95,\n",
    "    sgd_minibatch_size=32,\n",
    "    train_batch_size=128,\n",
    "    # lambda_=tune.grid_search([0.9, 0.95, 0.999]),\n",
    "    # sgd_minibatch_size=tune.grid_search([32, 64, 128]),\n",
    "    # train_batch_size=tune.grid_search([32, 64, 128]),\n",
    "    # seed=14,\n",
    "    model={\n",
    "        \"fcnet_hiddens\": [256, 256],\n",
    "    },\n",
    ")\n",
    "ppo_config = ppo_config.environment(\n",
    "    env=CustomizedGrid2OpEnvironment,\n",
    "    env_config={\n",
    "        \"env_name\": ENV_NAME,\n",
    "        \"num_agents\": len(policies),\n",
    "        \"action_space\": \"tennet\",\n",
    "        \"lib_dir\": LIB_DIR,\n",
    "        \"max_tsteps\": NB_TSTEPS,\n",
    "        \"grid2op_kwargs\": {\n",
    "            \"test\": ENV_IS_TEST,\n",
    "            # \"reward_class\": Reward.L2RPNReward,\n",
    "            \"reward_class\": LossReward,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "ppo_config.multi_agent(\n",
    "    policies=policies,\n",
    "    policy_mapping_fn=policy_mapping_fn,\n",
    "    policies_to_train=[\"reinforcement_learning_policy\"],\n",
    ")\n",
    "\n",
    "ppo_config.framework(framework=\"torch\")\n",
    "ppo_config.rl_module(_enable_rl_module_api=False)\n",
    "ppo_config.exploration(\n",
    "    exploration_config={\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "    }\n",
    ")\n",
    "ppo_config.callbacks(CustomMetricsCallback)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to YAML string\n",
    "yaml_string = yaml.dump(ppo_config)\n",
    "\n",
    "# Write YAML string to a file\n",
    "with open(\"ppo_config.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(yaml_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barberademol/Documents/GitHub/mahrl_grid2op/venv_mahrl/lib/python3.10/site-packages/grid2op/MakeEnv/Make.py:420: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_space': Discrete(2), 'observation_space': 'None', 'config': <ray.rllib.algorithms.algorithm_config.AlgorithmConfig object at 0x2898016f0>}\n"
     ]
    }
   ],
   "source": [
    "from numpy import save\n",
    "import yaml\n",
    "from ray import tune\n",
    "    \n",
    "from gymnasium.spaces.discrete import Discrete\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "# Custom constructor for Discrete\n",
    "def discrete_constructor(loader, node):\n",
    "    return Discrete(int(loader.construct_scalar(node)))\n",
    "\n",
    "# Custom constructor for AlgorithmConfig\n",
    "def algorithm_config_constructor(loader, node):\n",
    "    fields = loader.construct_mapping(node)\n",
    "    return AlgorithmConfig()\n",
    "\n",
    "# Custom constructor for PolicySpec\n",
    "def policy_spec_constructor(loader, node):\n",
    "    fields = loader.construct_mapping(node)\n",
    "    return PolicySpec()\n",
    "\n",
    "# Custom constructor for CustomizedGrid2OpEnvironment\n",
    "def customized_environment_constructor(loader, node):\n",
    "    fields = loader.construct_mapping(node, deep=True)\n",
    "    env_config = fields.get('env_config', {})  # Extract env_config explicitly\n",
    "    fields['env_config'] = env_config\n",
    "    return CustomizedGrid2OpEnvironment(**fields)\n",
    "\n",
    "# Custom constructor for LossReward\n",
    "def loss_reward_constructor(loader, node):\n",
    "    return LossReward()\n",
    "\n",
    "# Custom constructor for policy_mapping_fn\n",
    "def policy_mapping_fn_constructor(loader, node):\n",
    "    return policy_mapping_fn\n",
    "\n",
    "# Custom constructor for CustomMetricsCallback\n",
    "def custom_metrics_callback_constructor(loader, node):\n",
    "    return CustomMetricsCallback\n",
    "\n",
    "# Custom constructor for SelectAgentPolicy\n",
    "def select_agent_policy_constructor(loader, node):\n",
    "    fields = loader.construct_mapping(node)\n",
    "    return SelectAgentPolicy(**fields)\n",
    "\n",
    "# Custom constructor for DoNothingPolicy\n",
    "def do_nothing_policy_constructor(loader, node):\n",
    "    fields = loader.construct_mapping(node)\n",
    "    return DoNothingPolicy(**fields)\n",
    "\n",
    "# Add the constructors to the yaml loader\n",
    "yaml.add_constructor('!CustomizedGrid2OpEnvironment', customized_environment_constructor)\n",
    "yaml.add_constructor('!LossReward', loss_reward_constructor)\n",
    "yaml.add_constructor('!policy_mapping_fn', policy_mapping_fn_constructor)\n",
    "yaml.add_constructor('!CustomMetricsCallback', custom_metrics_callback_constructor)\n",
    "yaml.add_constructor('!SelectAgentPolicy', select_agent_policy_constructor)\n",
    "yaml.add_constructor('!DoNothingPolicy', do_nothing_policy_constructor)\n",
    "yaml.add_constructor('!Discrete', discrete_constructor)\n",
    "yaml.add_constructor('!AlgorithmConfig', algorithm_config_constructor)\n",
    "yaml.add_constructor('!PolicySpec', policy_spec_constructor)\n",
    "\n",
    "config = yaml.load(open(\"/Users/barberademol/Documents/GitHub/mahrl_grid2op/experiments/configurations/ppo_baseline.yaml\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mapper = {\n",
    "#     \"callbacks\":{\n",
    "#         \"LogDistributionsCallback\": LogDistributionsCallback\n",
    "#     },\n",
    "#     \"env\":{\n",
    "#         \"Grid_Gym\": Grid_Gym\n",
    "#     }\n",
    "# }\n",
    "# def preprocess_config(config):\n",
    "#     \"\"\"\n",
    "#     Transform the string representations of classes in YAML\n",
    "#     files to the corresponding python objects.\n",
    "\n",
    "#     Args:\n",
    "#         config (dict): parsed YAML config file\n",
    "#     \"\"\"\n",
    "#     if \"callbacks\" in config[\"tune_config\"]:\n",
    "#         config[\"tune_config\"][\"callbacks\"] = mapper[\"callbacks\"][config[\"tune_config\"][\"callbacks\"]]\n",
    "#     try:\n",
    "#         config[\"tune_config\"][\"env\"] = mapper[\"env\"][config[\"tune_config\"][\"env\"]]\n",
    "#     except: # if the env is not in the mapper, it is an already registerd env\n",
    "#         pass\n",
    "\n",
    "\n",
    "#     return config\n",
    "\n",
    "# config = preprocess_config(yaml.load(open(args.algorithm_config_path), Loader=get_loader()))[\"tune_config\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mahrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
