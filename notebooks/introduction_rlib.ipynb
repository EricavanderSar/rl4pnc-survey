{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ray.rllib.algorithms import ppo\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up checkpoints path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_ROOT = \"tmp/ppo/cart\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = os.getenv(\"HOME\") + \"/ray_results/\"\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 17:17:27,270\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-11-02 17:17:27,271\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-11-02 17:17:27,271\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='CartPole-v1', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('CartPole-v1').build()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=4381)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-11-02 17:17:29,397\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-11-02 17:17:29,407\tWARNING util.py:68 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "SELECT_ENV = \"CartPole-v1\"\n",
    "\n",
    "config = ppo.PPOConfig()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "agent = ppo.PPO(config, env=SELECT_ENV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0: min=  8.00/mean= 37.35/max= 91.00\n",
      "Iteration  1: min=  8.00/mean= 56.43/max=258.00\n",
      "Iteration  2: min= 13.00/mean= 85.71/max=271.00\n",
      "Iteration  3: min= 13.00/mean=112.78/max=500.00\n",
      "Iteration  4: min= 13.00/mean=148.93/max=500.00\n",
      "Iteration  5: min= 13.00/mean=177.39/max=500.00\n",
      "Iteration  6: min= 13.00/mean=210.97/max=500.00\n",
      "Iteration  7: min= 13.00/mean=246.14/max=500.00\n",
      "Iteration  8: min= 13.00/mean=278.51/max=500.00\n",
      "Iteration  9: min= 18.00/mean=313.37/max=500.00\n",
      "Iteration 10: min= 18.00/mean=341.54/max=500.00\n",
      "Iteration 11: min= 18.00/mean=354.00/max=500.00\n",
      "Iteration 12: min= 82.00/mean=369.89/max=500.00\n",
      "Iteration 13: min= 82.00/mean=386.41/max=500.00\n",
      "Iteration 14: min= 82.00/mean=392.92/max=500.00\n",
      "Iteration 15: min=110.00/mean=399.31/max=500.00\n",
      "Iteration 16: min=110.00/mean=401.04/max=500.00\n",
      "Iteration 17: min=110.00/mean=401.25/max=500.00\n",
      "Iteration 18: min=110.00/mean=404.21/max=500.00\n",
      "Iteration 19: min=110.00/mean=404.82/max=500.00\n",
      "Iteration 20: min=110.00/mean=413.43/max=500.00\n",
      "Iteration 21: min=110.00/mean=430.13/max=500.00\n",
      "Iteration 22: min=110.00/mean=450.08/max=500.00\n",
      "Iteration 23: min=138.00/mean=470.91/max=500.00\n",
      "Iteration 24: min=138.00/mean=473.62/max=500.00\n",
      "Iteration 25: min=138.00/mean=475.34/max=500.00\n",
      "Iteration 26: min=138.00/mean=477.11/max=500.00\n",
      "Iteration 27: min=218.00/mean=484.73/max=500.00\n",
      "Iteration 28: min=218.00/mean=486.20/max=500.00\n",
      "Iteration 29: min=218.00/mean=489.57/max=500.00\n",
      "Iteration 30: min=218.00/mean=490.43/max=500.00\n",
      "Iteration 31: min=218.00/mean=491.76/max=500.00\n",
      "Iteration 32: min=218.00/mean=493.53/max=500.00\n",
      "Iteration 33: min=341.00/mean=496.35/max=500.00\n",
      "Iteration 34: min=341.00/mean=496.35/max=500.00\n",
      "Iteration 35: min=390.00/mean=497.94/max=500.00\n",
      "Iteration 36: min=390.00/mean=496.98/max=500.00\n",
      "Iteration 37: min=390.00/mean=496.98/max=500.00\n",
      "Iteration 38: min=355.00/mean=492.64/max=500.00\n",
      "Iteration 39: min=355.00/mean=492.64/max=500.00\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 40\n",
    "s = \"Iteration{:3d}: min={:6.2f}/mean={:6.2f}/max={:6.2f}\"\n",
    "\n",
    "for n in range(N_ITER):\n",
    "  result = agent.train()\n",
    "  file_name = agent.save(CHECKPOINT_ROOT)\n",
    "\n",
    "  print(s.format(\n",
    "    n,\n",
    "    result[\"episode_reward_min\"],\n",
    "    result[\"episode_reward_mean\"],\n",
    "    result[\"episode_reward_max\"],\n",
    "    result[\"episode_len_mean\"],\n",
    "    file_name\n",
    "   ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mahrl",
   "language": "python",
   "name": "venv_mahrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
