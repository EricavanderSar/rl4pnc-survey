# training
nb_timesteps: &nb_timesteps 1000000
checkpoint_freq: 10000
verbose: 1
gamma: 0.95 # !grid_search [0.95, 0.99]
lr: 0.003 # !grid_search [0.0005, 0.005, 0.05]
vf_loss_coeff: 0.5
entropy_coeff: 0.01
clip_param: 0.2
lambda_: 0.95
sgd_minibatch_size: 32 # !grid_search [32, 64, 128]
train_batch_size: 128
model:
  fcnet_hiddens: 
  - 256
  - 256
_enable_learner_api: False
seed: &seed 14
  
# environment
env_config:
  env_name: rte_case5_example
  num_agents: 3
  action_space: asymmetry
  lib_dir: /Users/barberademol/Documents/GitHub/mahrl_grid2op/
  max_tsteps: *nb_timesteps
  grid2op_kwargs:
    test: True
    reward_class: !LossReward
  seed: *seed
  rho_threshold: 0.9

# multi agent
policy_mapping_fn: !policy_mapping_fn
policies_to_train: ["reinforcement_learning_policy"]

# framework
framework: torch

# rl module
_enable_rl_module_api: False

# exploration
exploration_config:
  type: EpsilonGreedy

# callbacks
callbacks: !CustomMetricsCallback